{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KcuEdH16qPSj",
        "6EOauScSh4jS",
        "I_AnpK2ejgsa",
        "imzlAhI2kVV1",
        "_95XJGR7B5vV",
        "Y_rNMrV3qn8d",
        "0aZmFVCN_xKh"
      ],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "BRF1sS3-qK9g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Swfarmg1Dp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import time\n",
        "import zipfile\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import glob\n",
        "import csv\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "QBikEyvOEXgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUNCTIONS"
      ],
      "metadata": {
        "id": "RvgERTjoq8cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_detections(pkl_path):\n",
        "    \"\"\"Loads model prediction dictionary from a pickle file.\"\"\"\n",
        "    try:\n",
        "        # Open the pickle file to get the stored outputs\n",
        "        with open(pkl_path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            # Detections are stored under the 'outputs' key\n",
        "            return data.get('outputs', {})\n",
        "    except (FileNotFoundError, pickle.UnpicklingError, KeyError) as e:\n",
        "        # Handle cases where the file is missing, corrupt, or has the wrong format\n",
        "        print(f\"Error loading detections from {pkl_path}: {e}\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "siV_uZ3LaoQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_bin_collage(bin_df, bin_label, metric_col, data, metric_name=None, num_images=3, cols=3, title_prefix=\"Bin\", target_size=1024):\n",
        "    \"\"\"Creates a collage of sample images from a specific data bin.\"\"\"\n",
        "\n",
        "    # Take a random, consistent sample of images from the provided dataframe\n",
        "    sample_df = bin_df.sample(min(num_images, len(bin_df)), random_state=42)\n",
        "\n",
        "    # Set up the plot grid\n",
        "    rows = (len(sample_df) + cols - 1) // cols\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Default to using the metric column name if a display name isn't provided\n",
        "    metric_name = metric_name or metric_col\n",
        "\n",
        "    # Hide any unused subplots in the grid\n",
        "    for ax in axes[len(sample_df):]:\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Loop through the sampled images and plot them\n",
        "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
        "        img_name = os.path.basename(row['image_name'])\n",
        "        dataset_name = row['original_dataset']\n",
        "        metric_val = row[metric_col]\n",
        "\n",
        "        # Get metadata for this image (path, GT boxes, etc.)\n",
        "        info = get_dataset_info_by_name(img_name, data)\n",
        "        if not info:\n",
        "            print(f\"Unknown dataset for image: {img_name}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Load, resize, and display the image\n",
        "            img = Image.open(info['path']).convert(\"RGB\").resize((target_size, target_size))\n",
        "            ax = axes[i]\n",
        "            ax.imshow(np.array(img))\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{img_name}\\n{metric_name}: {metric_val:.1f} | {dataset_name}\", fontsize=10)\n",
        "\n",
        "            # Scale the ground truth boxes to match the resized image\n",
        "            scaled_boxes = scale_boxes(info['gt_boxes'], info['width'], info['height'], target_size, target_size)\n",
        "\n",
        "            # Draw the ground truth boxes on the image\n",
        "            for box in scaled_boxes:\n",
        "                x1, y1, x2, y2 = box\n",
        "                rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "                ax.add_patch(rect)\n",
        "        except FileNotFoundError:\n",
        "            # Handle cases where the image file is missing\n",
        "            print(f\"Image not found at path: {info['path']}\")\n",
        "            axes[i].set_title(f\"{img_name}\\n(Image not found)\", fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "    # Add a main title to the entire collage\n",
        "    plt.suptitle(f\"{title_prefix}: {bin_label}\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6ZbhIFwy-bEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset_info_by_name(img_name_cleaned, data):\n",
        "    \"\"\"\n",
        "    Looks up an image's metadata (path, GT boxes, dimensions)\n",
        "    by searching across all loaded datasets.\n",
        "    \"\"\"\n",
        "    # Loop through each dataset config (synthetic, nominal, edge)\n",
        "    for dtype, config in DATASET_CONFIGS.items():\n",
        "        df = data[dtype]['df']\n",
        "\n",
        "        # Check if the image name exists in this dataset's dataframe\n",
        "        # Using .values is a quick way to search the series\n",
        "        if img_name_cleaned in df['image_name'].values:\n",
        "            # If found, get the corresponding row for this image\n",
        "            row = df[df['image_name'] == img_name_cleaned].iloc[0]\n",
        "\n",
        "            # Return a dictionary with all the necessary info\n",
        "            return {\n",
        "                \"img_path\": os.path.join(config['image_folder'], img_name_cleaned),\n",
        "                \"gt_boxes\": data[dtype]['gt'].get(img_name_cleaned, []),\n",
        "                \"width\": row['width'],\n",
        "                \"height\": row['height']\n",
        "            }\n",
        "\n",
        "    # If the loop finishes without finding the image, return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "1Gjt8Y_G383T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_quad_area(row):\n",
        "    \"\"\"Calculates the area of a quadrilateral from its corner coordinates.\"\"\"\n",
        "\n",
        "    # Unpack the four corner points (A, B, C, D) from the input row\n",
        "    x1, y1 = row['x_A'], row['y_A']\n",
        "    x2, y2 = row['x_B'], row['y_B']\n",
        "    x3, y3 = row['x_C'], row['y_C']\n",
        "    x4, y4 = row['x_D'], row['y_D']\n",
        "\n",
        "    # Apply the Shoelace formula for the area of a simple polygon\n",
        "    return 0.5 * abs(\n",
        "        x1*y2 + x2*y3 + x3*y4 + x4*y1\n",
        "        - y1*x2 - y2*x3 - y3*x4 - y4*x1\n",
        "    )"
      ],
      "metadata": {
        "id": "-bgXxs4tgtXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_multiple_detections(outputs, score_thresh=0.90, detailed=False):\n",
        "    \"\"\"\n",
        "    Finds images that have more than one detection above a certain score threshold.\n",
        "    This is useful for identifying duplicate detections.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Go through each image's predictions in the outputs dictionary\n",
        "    for img_name, pred in outputs.items():\n",
        "        scores = pred['scores']\n",
        "        boxes = pred['boxes']\n",
        "\n",
        "        # Create a boolean mask to find predictions with high enough scores\n",
        "        high_score_mask = scores >= score_thresh\n",
        "        high_score_boxes = boxes[high_score_mask]\n",
        "        high_scores = scores[high_score_mask]\n",
        "\n",
        "        # If more than one high-confidence box was found, it's a case of multiple detections\n",
        "        if len(high_score_boxes) > 1:\n",
        "            # The 'detailed' flag controls how much info we save\n",
        "            if detailed:\n",
        "                # Save the image name along with the actual boxes and scores\n",
        "                results.append((img_name, high_score_boxes, high_scores))\n",
        "            else:\n",
        "                # Otherwise, just save the count of how many extra boxes there were\n",
        "                results.append((img_name, len(high_score_boxes)))\n",
        "\n",
        "    # Return the list of images that met the criteria, and the total count\n",
        "    return results, len(results)"
      ],
      "metadata": {
        "id": "AEPZaIPztSIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ground_truth(gt_folder, image_extension=\"png\"):\n",
        "    \"\"\"\n",
        "    Loads ground truth bounding boxes from text files in a specified folder.\n",
        "    Each text file corresponds to an image and contains bounding box data.\n",
        "    \"\"\"\n",
        "    # Make sure the image extension has a leading dot\n",
        "    if not image_extension.startswith(\".\"):\n",
        "        image_extension = \".\" + image_extension\n",
        "\n",
        "    gt_data = {}\n",
        "    # Find all .txt files in the ground truth folder\n",
        "    txt_files = glob.glob(os.path.join(gt_folder, \"*.txt\"))\n",
        "\n",
        "    # Process each ground truth file\n",
        "    for txt_file in txt_files:\n",
        "        # Infer the corresponding image name from the text file's name\n",
        "        image_name, _ = os.path.splitext(os.path.basename(txt_file))\n",
        "        image_name += image_extension\n",
        "\n",
        "        boxes = []\n",
        "        try:\n",
        "            with open(txt_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    # Expecting format: class_id x1 y1 x2 y2\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) == 5:\n",
        "                        try:\n",
        "                            # Unpack the coordinates, ignoring the class_id\n",
        "                            _, x1, y1, x2, y2 = map(float, parts)\n",
        "                            # Standardize the box format to (xmin, ymin, xmax, ymax)\n",
        "                            boxes.append([min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)])\n",
        "                        except ValueError:\n",
        "                            # Skip lines with non-numeric coordinate values\n",
        "                            pass\n",
        "            if boxes:\n",
        "                # Map the list of boxes to the corresponding image name\n",
        "                gt_data[image_name] = boxes\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading GT file {txt_file}: {e}\")\n",
        "\n",
        "    return gt_data"
      ],
      "metadata": {
        "id": "lKn6TH0YtZCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_boxes(boxes, orig_w, orig_h, target_w, target_h):\n",
        "    \"\"\"\n",
        "    Scales bounding boxes from an original image size to a new target size.\n",
        "    Useful for converting between original image coordinates and model input coordinates.\n",
        "    \"\"\"\n",
        "    # Avoid division by zero if original dimensions are invalid\n",
        "    if orig_w == 0 or orig_h == 0:\n",
        "        return []\n",
        "\n",
        "    # Calculate the scaling factor for width and height\n",
        "    x_scale = target_w / orig_w\n",
        "    y_scale = target_h / orig_h\n",
        "\n",
        "    # Apply the scaling factor to each coordinate of each box\n",
        "    return [[b[0] * x_scale, b[1] * y_scale, b[2] * x_scale, b[3] * y_scale] for b in boxes]"
      ],
      "metadata": {
        "id": "nL-x_hNnrZIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_predictions_with_gt(img_path, gt_boxes_orig, pred_boxes, pred_scores, orig_w, orig_h, ax, dataset_name, score_threshold=0.90, target_size=1024):\n",
        "    \"\"\"\n",
        "    Draws ground truth boxes and the model's filtered predictions on an image.\n",
        "    \"\"\"\n",
        "    # Load and resize the image for consistent display\n",
        "    img = Image.open(img_path).convert(\"RGB\").resize((target_size, target_size))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"{os.path.basename(img_path)}\\n{dataset_name}\", fontsize=9)\n",
        "\n",
        "    # Draw Ground Truth boxes in green\n",
        "    if gt_boxes_orig:\n",
        "        # Scale GT boxes from original image coordinates to the displayed size\n",
        "        gt_boxes_scaled = scale_boxes(gt_boxes_orig, orig_w, orig_h, target_size, target_size)\n",
        "        for box in gt_boxes_scaled:\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "    # Filter and draw the model's prediction boxes in red\n",
        "    for i, box in enumerate(pred_boxes):\n",
        "        # Only draw boxes that are above the confidence score threshold\n",
        "        if pred_scores[i] >= score_threshold:\n",
        "            x1, y1, x2, y2 = box\n",
        "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, linestyle='--', edgecolor='red', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            # Add the confidence score as a text label on the box\n",
        "            ax.text(x1, y1 - 5, f'{pred_scores[i]:.2f}', bbox=dict(facecolor='red', alpha=0.5), fontsize=7, color='white')"
      ],
      "metadata": {
        "id": "AVIPwy7403lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_with_dataset(multi_list, tag):\n",
        "    \"\"\"Appends a dataset tag to each item in a list of detections.\"\"\"\n",
        "\n",
        "    # Uses a list comprehension to iterate through the list and add the tag\n",
        "    # Input format: [(img, boxes, scores), ...]\n",
        "    # Output format: [(img, boxes, scores, tag), ...]\n",
        "    return [(img, boxes, scores, tag) for (img, boxes, scores) in multi_list]"
      ],
      "metadata": {
        "id": "uQeCeWhm-ind"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(box1, box2):\n",
        "    \"\"\"Calculates the Intersection over Union (IoU) for two bounding boxes.\"\"\"\n",
        "\n",
        "    # box format is [x1, y1, x2, y2]\n",
        "\n",
        "    # Determine the coordinates of the intersection rectangle\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    # Compute the area of intersection, adding 1 for pixel-based coordinates\n",
        "    # The max(0, ...) ensures the area is non-negative if boxes don't overlap\n",
        "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\n",
        "    # Compute the area of both the prediction and ground-truth rectangles\n",
        "    box1Area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    box2Area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "    # Calculate the IoU by dividing the intersection area by the union area\n",
        "    # Union Area = Box1 Area + Box2 Area - Intersection Area\n",
        "    iou = interArea / float(box1Area + box2Area - interArea)\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "BULox4lWyEX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_dataset(dataset_type, iou_threshold=0.3, score_threshold=0.90, return_detailed=False):\n",
        "    \"\"\"\n",
        "    Calculates performance metrics (TP, FP, FN, etc.) for a given dataset.\n",
        "    Can also return a detailed, per-image breakdown of results.\n",
        "    \"\"\"\n",
        "    # Grab all the necessary data for this dataset from our main data dictionary\n",
        "    dataset_info = data[dataset_type]\n",
        "    gt_data = dataset_info['gt']\n",
        "    predictions = dataset_info['outputs']\n",
        "    df = dataset_info['df']\n",
        "    # Create a quick lookup for image dimensions for scaling\n",
        "    image_dims_lookup = dict(zip(df['image_name'], zip(df['width'], df['height'])))\n",
        "\n",
        "    # Initialize counters for the evaluation metrics\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    detailed_results = []\n",
        "\n",
        "    # Get a set of all unique image names across both ground truth and predictions\n",
        "    all_image_names = set(gt_data.keys()) | set(predictions.keys())\n",
        "\n",
        "    # Go through each image to evaluate its predictions\n",
        "    for img_name in all_image_names:\n",
        "        # Get ground truth and predictions for the current image\n",
        "        gt_boxes_orig = gt_data.get(img_name, [])\n",
        "        pred_dict = predictions.get(img_name, {})\n",
        "        pred_scores = pred_dict.get('scores', [])\n",
        "        pred_boxes = pred_dict.get('boxes', [])\n",
        "\n",
        "        # Filter predictions to only include those above the score threshold\n",
        "        high_score_boxes = [box for i, box in enumerate(pred_boxes) if pred_scores[i] >= score_threshold]\n",
        "\n",
        "        # Store detailed results for this image, assuming all preds are FP and all GTs are FN initially\n",
        "        image_result = {\n",
        "            'image_name': img_name,\n",
        "            'dataset_name': df[df['image_name'] == img_name]['original_dataset'].iloc[0] if not df[df['image_name'] == img_name].empty else 'Unknown',\n",
        "            'predicted_boxes': high_score_boxes,\n",
        "            'ground_truth_boxes': gt_boxes_orig,\n",
        "            'is_predicted_box_tp': [False] * len(high_score_boxes),\n",
        "            'is_gt_box_fn': [True] * len(gt_boxes_orig)\n",
        "        }\n",
        "\n",
        "        orig_w, orig_h = image_dims_lookup.get(img_name, (0, 0))\n",
        "\n",
        "        # Scale GT boxes to the model's coordinate space (1024x1024) for comparison\n",
        "        scaled_gt_boxes = scale_boxes(gt_boxes_orig, orig_w, orig_h, 1024, 1024)\n",
        "\n",
        "        # Keep track of which GT boxes have already been matched to a prediction\n",
        "        matched_gt_indices = set()\n",
        "        # Iterate through each prediction to find a matching GT box\n",
        "        for pred_idx, pred_box in enumerate(high_score_boxes):\n",
        "            best_iou, best_gt_idx = -1, -1\n",
        "            # Find the best GT box match for the current prediction\n",
        "            for gt_idx, gt_box in enumerate(scaled_gt_boxes):\n",
        "                iou = compute_iou(pred_box, gt_box)\n",
        "                if iou > best_iou:\n",
        "                    best_iou, best_gt_idx = iou, gt_idx\n",
        "\n",
        "            # If a good, unmatched GT box is found, count it as a True Positive\n",
        "            if best_iou >= iou_threshold and best_gt_idx not in matched_gt_indices:\n",
        "                TP += 1\n",
        "                matched_gt_indices.add(best_gt_idx)\n",
        "                image_result['is_predicted_box_tp'][pred_idx] = True  # Mark prediction as TP\n",
        "                if best_gt_idx < len(image_result['is_gt_box_fn']):\n",
        "                    image_result['is_gt_box_fn'][best_gt_idx] = False # Mark GT as \"found\"\n",
        "            else:\n",
        "                # Otherwise, the prediction is a False Positive\n",
        "                FP += 1\n",
        "\n",
        "        # Any GT boxes that were never matched are False Negatives\n",
        "        FN += len(scaled_gt_boxes) - len(matched_gt_indices)\n",
        "\n",
        "        # Store the detailed breakdown if the user requested it\n",
        "        if return_detailed:\n",
        "            detailed_results.append(image_result)\n",
        "\n",
        "    if return_detailed:\n",
        "        return detailed_results\n",
        "\n",
        "    # Calculate final precision, recall, and F1 score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else"
      ],
      "metadata": {
        "id": "Zb3M0ElIyFQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_sensitivity_analysis(sweep_param, fixed_param_value, sweep_range):\n",
        "    \"\"\"\n",
        "    Runs a sensitivity analysis for a given parameter (IoU or score) across all datasets.\n",
        "    This helps find the optimal thresholds by testing a range of values.\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # Loop through each dataset (synthetic, nominal, edge)\n",
        "    for dataset_type, config in DATASET_CONFIGS.items():\n",
        "        print(f\"Running {sweep_param} sweep for {config['name']}...\")\n",
        "        # Loop through the provided range of threshold values\n",
        "        for threshold_val in sweep_range:\n",
        "\n",
        "            # Call the main evaluation function with the correct parameter being swept\n",
        "            if sweep_param == 'iou':\n",
        "                # Sweep IoU, hold score constant\n",
        "                result = evaluate_dataset(dataset_type, iou_threshold=threshold_val, score_threshold=fixed_param_value)\n",
        "            elif sweep_param == 'score':\n",
        "                # Sweep score, hold IoU constant\n",
        "                result = evaluate_dataset(dataset_type, iou_threshold=fixed_param_value, score_threshold=threshold_val)\n",
        "            else:\n",
        "                raise ValueError(\"sweep_param must be 'iou' or 'score'\")\n",
        "\n",
        "            # Store the results in a structured list for later analysis\n",
        "            all_results.append({\n",
        "                'dataset': config['name'],\n",
        "                'sweep_param_value': threshold_val,\n",
        "                'precision': result['precision'],\n",
        "                'recall': result['recall'],\n",
        "                'f1_score': result['f1_score']\n",
        "            })\n",
        "\n",
        "    # Return a single pandas DataFrame containing all the collected results\n",
        "    return pd.DataFrame(all_results)"
      ],
      "metadata": {
        "id": "gIHXyd7h51aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sensitivity_results(results_df, sweep_param):\n",
        "    \"\"\"\n",
        "    Plots the results of a sensitivity analysis (e.g., precision, recall vs. threshold)\n",
        "    for multiple datasets in a single grid.\n",
        "    \"\"\"\n",
        "    # Reshape the DataFrame from a wide to a long format.\n",
        "    # This is necessary to use the different metrics (precision, recall, f1) as a 'hue' in seaborn.\n",
        "    df_melted = results_df.melt(\n",
        "        id_vars=['dataset', 'sweep_param_value'],\n",
        "        value_vars=['precision', 'recall', 'f1_score'],\n",
        "        var_name='metric',\n",
        "        value_name='score'\n",
        "    )\n",
        "\n",
        "    # Set up a FacetGrid, which creates a separate plot column for each dataset\n",
        "    g = sns.FacetGrid(df_melted, col=\"dataset\", hue=\"metric\", col_wrap=2, height=5)\n",
        "\n",
        "    # Map the lineplot function onto each subplot in the grid\n",
        "    g.map(sns.lineplot, \"sweep_param_value\", \"score\", marker='o').add_legend()\n",
        "\n",
        "    # Set the axis labels and titles for clarity\n",
        "    g.set_axis_labels(f'{sweep_param.capitalize()} Threshold', 'Score')\n",
        "    g.set_titles(\"{col_name}\") # Use the dataset name as the subplot title\n",
        "    plt.ylim(-0.05, 1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q8CIwWiL8GvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING FILES"
      ],
      "metadata": {
        "id": "KcuEdH16qPSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_CONFIGS = {\n",
        "    'synthetic': {\n",
        "        'csv_path': \"/content/drive/MyDrive/Dataset/synthetic_with_detection_flags.csv\",\n",
        "        'pkl_path': '/content/drive/MyDrive/test_outputs_synthetic.pkl',\n",
        "        'image_folder': \"/content/drive/MyDrive/Dataset/data_tlbr/TestSynth/images\",\n",
        "        'gt_folder': '/content/drive/MyDrive/Dataset/data_tlbr/TestSynth/labels',\n",
        "        'image_ext': \".jpeg\",\n",
        "        'name': 'LARD_test_synth'\n",
        "    },\n",
        "    'nominal': {\n",
        "        'csv_path': \"/content/drive/MyDrive/Dataset/real_nominal_with_detection_flags.csv\",\n",
        "        'pkl_path': '/content/drive/MyDrive/test_outputs_real_nominal.pkl',\n",
        "        'image_folder': \"/content/drive/MyDrive/Dataset/data_tlbr/Test_RealNominal/images\",\n",
        "        'gt_folder': '/content/drive/MyDrive/Dataset/data_tlbr/Test_RealNominal/labels',\n",
        "        'image_ext': \".png\",\n",
        "        'name': 'REAL_Nominal'\n",
        "    },\n",
        "    'edge': {\n",
        "        'csv_path': \"/content/drive/MyDrive/Dataset/real_edge_with_detection_flags.csv\",\n",
        "        'pkl_path': '/content/drive/MyDrive/test_outputs_real_edge.pkl',\n",
        "        'image_folder': \"/content/drive/MyDrive/Dataset/data_tlbr/Test_RealEdge/images\",\n",
        "        'gt_folder': '/content/drive/MyDrive/Dataset/data_tlbr/Test_RealEdge/labels',\n",
        "        'image_ext': \".png\",\n",
        "        'name': 'REAL_Edge_Cases'\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "fmjnXYHeah5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "881a1990"
      },
      "source": [
        "data = {}\n",
        "for dtype, config in DATASET_CONFIGS.items():\n",
        "    print(f\"Loading {dtype} data...\")\n",
        "    data[dtype] = {\n",
        "        'df': pd.read_csv(config['csv_path']),\n",
        "        'outputs': load_detections(config['pkl_path']),\n",
        "        'gt': load_ground_truth(config['gt_folder'], config['image_ext'])\n",
        "    }\n",
        "print(\"All data loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Threshold Analysis"
      ],
      "metadata": {
        "id": "KZ_qcezbpnVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect all prediction scores from every image across all datasets\n",
        "all_scores = []\n",
        "for dataset_type in DATASET_CONFIGS:\n",
        "    outputs = data[dataset_type]['outputs']\n",
        "    for img_name, preds in outputs.items():\n",
        "        all_scores.extend(preds.get('scores', []))\n",
        "\n",
        "# Convert the list of scores into a pandas DataFrame for easy analysis\n",
        "scores_df = pd.DataFrame(all_scores, columns=['score'])\n",
        "\n",
        "# Create the distribution plot using a histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(data=scores_df, x='score', bins=50, kde=True)\n",
        "plt.title('Distribution of All Prediction Confidence Scores', fontsize=16)\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Number of Detections')\n",
        "# Add a vertical line to mark the start of our analysis range\n",
        "plt.axvline(x=0.90, color='r', linestyle='--', linewidth=2, label='Analysis Range Start (0.90)')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics to quantify the score distribution\n",
        "print(f\"Total number of detections (all scores): {len(scores_df)}\")\n",
        "print(f\"Number of detections with score >= 0.90: {len(scores_df[scores_df['score'] >= 0.90])}\")\n",
        "high_score_percentage = (len(scores_df[scores_df['score'] >= 0.90]) / len(scores_df)) * 100\n",
        "print(f\"Percentage of detections with score >= 0.90: {high_score_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "SgLtCDZpPnR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the different IoU thresholds and score thresholds we want to test\n",
        "iou_thresholds = [0.3, 0.5, 0.7]\n",
        "score_thresholds_to_test = np.arange(0.90, 1.00, 0.01)\n",
        "comprehensive_results = []\n",
        "\n",
        "# Run the evaluation in a nested loop to test every combination\n",
        "for iou in iou_thresholds:\n",
        "    print(f\"--- Running analysis for IoU Threshold: {iou} ---\")\n",
        "    # Loop through each dataset\n",
        "    for dataset_type, config in DATASET_CONFIGS.items():\n",
        "        # Loop through each score threshold\n",
        "        for score in score_thresholds_to_test:\n",
        "            # Call our main evaluation function with the current combination of thresholds\n",
        "            result = evaluate_dataset(dataset_type, iou_threshold=iou, score_threshold=score)\n",
        "\n",
        "            # Store the results in a list of dictionaries for easy conversion to a DataFrame\n",
        "            comprehensive_results.append({\n",
        "                'iou_threshold': iou,\n",
        "                'score_threshold': score,\n",
        "                'dataset': config['name'],\n",
        "                'precision': float(result['precision']), # Make sure data is numeric for plotting\n",
        "                'recall': float(result['recall']),\n",
        "                'f1_score': float(result['f1_score'])\n",
        "            })\n",
        "\n",
        "# Convert the list of results into a single pandas DataFrame\n",
        "results_df = pd.DataFrame(comprehensive_results)\n",
        "print(\"\\nComprehensive analysis complete.\")\n",
        "print(results_df.head())"
      ],
      "metadata": {
        "id": "V1irZyZZKTJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating Precision-Recall Curves ---\")\n",
        "\n",
        "# Use FacetGrid to create a separate plot for each dataset\n",
        "g_pr = sns.FacetGrid(results_df, col=\"dataset\", hue=\"iou_threshold\", height=5, palette='viridis')\n",
        "\n",
        "# Map a line plot onto the grid\n",
        "g_pr.map(sns.lineplot, \"recall\", \"precision\", marker='.', markersize=10).add_legend()\n",
        "\n",
        "# Set labels and titles\n",
        "g_pr.set_axis_labels(\"Recall\", \"Precision\")\n",
        "g_pr.set_titles(\"Dataset: {col_name}\")\n",
        "fig = g_pr.fig\n",
        "fig.suptitle(\"Precision-Recall Curve Analysis\", y=1.03, fontsize=16) # Add a main title\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LD4cu7PsK9L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating F1 Score vs. Confidence Threshold Curves (Highlighting Best Overall) ---\")\n",
        "\n",
        "# Create the main plot grid using seaborn's relplot for a figure-level plot\n",
        "g_f1 = sns.relplot(\n",
        "    data=results_df,\n",
        "    x=\"score_threshold\",\n",
        "    y=\"f1_score\",\n",
        "    hue=\"iou_threshold\", # Color lines by IoU threshold\n",
        "    col=\"dataset\",       # Create a separate column for each dataset\n",
        "    kind=\"line\",         # Specify a line plot\n",
        "    marker='.',\n",
        "    markersize=10,\n",
        "    height=5,\n",
        "    aspect=1.2,\n",
        "    palette='viridis',\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "# Find the single best operating point (highest F1 score) for each dataset\n",
        "# idxmax() finds the index of the first occurrence of the maximum value\n",
        "idx = results_df.groupby(['dataset'])['f1_score'].idxmax()\n",
        "best_overall_points = results_df.loc[idx]\n",
        "\n",
        "\n",
        "# Manually iterate through each subplot (ax) and plot a star for the best point\n",
        "# This is a robust way to add annotations to a FacetGrid\n",
        "for ax, dataset_name in zip(g_f1.axes.flat, g_f1.col_names):\n",
        "\n",
        "    # Find the row in our 'best_overall_points' DataFrame that matches this subplot's dataset\n",
        "    point_to_plot = best_overall_points[best_overall_points['dataset'] == dataset_name]\n",
        "\n",
        "    # If we found a point, plot a single large star on this subplot\n",
        "    if not point_to_plot.empty:\n",
        "        # Get the coordinates for the star from our best_overall_points df\n",
        "        x = point_to_plot['score_threshold'].iloc[0]\n",
        "        y = point_to_plot['f1_score'].iloc[0]\n",
        "\n",
        "        # Use matplotlib's scatter to draw the star on the current axis\n",
        "        ax.scatter(x, y, marker='*', s=500, color='gold', edgecolor='black', zorder=10)\n",
        "\n",
        "\n",
        "# Add final labels and a main title to the figure\n",
        "g_f1.set_axis_labels(\"Confidence Score Threshold\", \"F1 Score\")\n",
        "g_f1.fig.suptitle(\"F1 Score vs. Confidence Threshold (Overall Best Point Highlighted)\", y=1.03, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Print the summary table showing the exact values for the best operating points\n",
        "print(\"\\n--- Optimal Operating Point for Each Dataset ---\")\n",
        "print(best_overall_points[['dataset', 'iou_threshold', 'score_threshold', 'f1_score', 'precision', 'recall']])"
      ],
      "metadata": {
        "id": "KEbjJc0ELB91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results per dataset"
      ],
      "metadata": {
        "id": "A-_fQhfTO3wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the mAP metric from torchmetrics, specifying the box format\n",
        "metric = MeanAveragePrecision(box_format='xyxy')\n",
        "\n",
        "# Create a dictionary to store the final results for each dataset\n",
        "all_map_results = {}\n",
        "\n",
        "# Loop through each dataset to calculate its mAP separately\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    dataset_name = config['name']\n",
        "    print(f\"--- Calculating mAP for: {dataset_name} ---\")\n",
        "\n",
        "    # Get the pre-loaded ground truth, predictions, and dataframe for this dataset\n",
        "    gt_data = data[dataset_type]['gt']\n",
        "    predictions = data[dataset_type]['outputs']\n",
        "    df = data[dataset_type]['df']\n",
        "\n",
        "    # Create a quick lookup for original image dimensions for scaling\n",
        "    image_dims_lookup = dict(zip(df['image_name'], zip(df['width'], df['height'])))\n",
        "\n",
        "    # These lists will hold the formatted data required by torchmetrics\n",
        "    preds_for_metric = []\n",
        "    targets_for_metric = []\n",
        "\n",
        "    # Iterate through every image that has a ground truth label\n",
        "    for image_name, gt_boxes in gt_data.items():\n",
        "        # Format the ground truth (target) boxes into the required tensor format\n",
        "        targets_for_metric.append({\n",
        "            'boxes': torch.tensor(gt_boxes, dtype=torch.float32),\n",
        "            'labels': torch.ones(len(gt_boxes), dtype=torch.int32) # Assign a single class label\n",
        "        })\n",
        "\n",
        "        # Get the predictions for this image, defaulting to empty if none exist\n",
        "        pred_dict = predictions.get(image_name, {})\n",
        "        pred_boxes_model_space = pred_dict.get('boxes', [])\n",
        "        pred_scores = pred_dict.get('scores', [])\n",
        "\n",
        "        # Get the original image dimensions to scale the predictions\n",
        "        orig_w, orig_h = image_dims_lookup.get(image_name, (0, 0))\n",
        "        if orig_w == 0 or orig_h == 0:\n",
        "            # If dimensions are missing, we can't scale, so use empty boxes\n",
        "            pred_boxes_orig_space = []\n",
        "        else:\n",
        "            # Scale prediction boxes from model space (1024x1024) back to original image space\n",
        "            pred_boxes_orig_space = scale_boxes(pred_boxes_model_space, 1024, 1024, orig_w, orig_h)\n",
        "\n",
        "        # Format the scaled predictions into the required tensor format\n",
        "        preds_for_metric.append({\n",
        "            'boxes': torch.tensor(pred_boxes_orig_space, dtype=torch.float32),\n",
        "            'scores': torch.tensor(pred_scores, dtype=torch.float32),\n",
        "            'labels': torch.ones(len(pred_boxes_orig_space), dtype=torch.int32)\n",
        "        })\n",
        "\n",
        "    # Update the metric with all the predictions and targets for this dataset\n",
        "    metric.update(preds_for_metric, targets_for_metric)\n",
        "    # Compute the final mAP scores\n",
        "    results = metric.compute()\n",
        "    all_map_results[dataset_name] = results\n",
        "    # Reset the metric to clear its state before processing the next dataset\n",
        "    metric.reset()\n",
        "\n",
        "# Print a summary table of the final results\n",
        "print(\"\\n\\n--- mAP Evaluation Summary ---\")\n",
        "summary_data = []\n",
        "for dataset_name, metrics in all_map_results.items():\n",
        "    summary_data.append({\n",
        "        'Dataset': dataset_name,\n",
        "        'mAP': f\"{metrics['map'].item():.4f}\",\n",
        "        'mAP_50': f\"{metrics['map_50'].item():.4f}\",\n",
        "        'mAP_75': f\"{metrics['map_75'].item():.4f}\",\n",
        "        'mAP (small)': f\"{metrics['map_small'].item():.4f}\",\n",
        "        'mAP (medium)': f\"{metrics['map_medium'].item():.4f}\",\n",
        "        'mAP (large)': f\"{metrics['map_large'].item():.4f}\",\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))"
      ],
      "metadata": {
        "id": "HOJQSXtp2a5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DETECTION RATES"
      ],
      "metadata": {
        "id": "fB7edPXji6hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting to update CSV files with 'detected' column ---\")\n",
        "\n",
        "# Define the confidence score threshold for a detection to be considered valid\n",
        "SCORE_THRESHOLD = 0.9\n",
        "\n",
        "# Loop through each dataset defined in our main config\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    dataset_name = config['name']\n",
        "    csv_path = config['csv_path']\n",
        "    print(f\"\\nProcessing dataset: {dataset_name}...\")\n",
        "\n",
        "    # Get the DataFrame and model predictions for the current dataset\n",
        "    df = data[dataset_type]['df']\n",
        "    outputs = data[dataset_type]['outputs']\n",
        "\n",
        "    # Create a lookup map of image_name -> True/False based on detection scores\n",
        "    detection_status = {}\n",
        "    for image_name, predictions in outputs.items():\n",
        "        # Check if any score in this image's prediction array is >= our threshold\n",
        "        if np.any(predictions.get('scores', []) >= SCORE_THRESHOLD):\n",
        "            detection_status[image_name] = True\n",
        "        else:\n",
        "            detection_status[image_name] = False\n",
        "\n",
        "    # Create the 'detected' column by mapping the status to each image name\n",
        "    # .fillna(False) handles images that are in the CSV but had no model output\n",
        "    df['detected'] = df['image_name'].map(detection_status).fillna(False)\n",
        "\n",
        "    # Save the modified DataFrame back to its original CSV file path\n",
        "    try:\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"Successfully updated and saved: {csv_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save {csv_path}. Error: {e}\")\n",
        "\n",
        "print(\"\\n--- All CSV files have been processed. ---\")"
      ],
      "metadata": {
        "id": "zmGhjYIvGJjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calculating detection rates for all datasets...\")\n",
        "\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    # Access the DataFrame from our central 'data' dictionary\n",
        "    df = data[dataset_type]['df']\n",
        "\n",
        "    # Check if the 'detected' column exists to avoid errors\n",
        "    if 'detected' in df.columns:\n",
        "        detection_rate = df['detected'].mean()\n",
        "        # Use the 'name' from our config for a nice, readable label\n",
        "        dataset_name = config.get('name', dataset_type.capitalize())\n",
        "        print(f\"Detection Rate ({dataset_name}): {detection_rate:.2%}\")\n",
        "    else:\n",
        "        print(f\"Warning: 'detected' column not found in '{dataset_type}' dataset.\")"
      ],
      "metadata": {
        "id": "IFeryifri_97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection Rate vs Slant Distance (Synthetic only)"
      ],
      "metadata": {
        "id": "6EOauScSh4jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_analysis = data['synthetic']['df'].copy()\n",
        "\n",
        "# The rest of your code works perfectly with this new df_analysis\n",
        "df_analysis[\"slant_distance\"] = pd.to_numeric(df_analysis[\"slant_distance\"], errors='coerce')\n",
        "df_analysis.dropna(subset=[\"slant_distance\"], inplace=True)\n",
        "\n",
        "# Bin slant distance\n",
        "num_bins = 10\n",
        "df_analysis[\"slant_bin\"] = pd.cut(df_analysis[\"slant_distance\"], bins=num_bins)\n",
        "\n",
        "# Group by bin and compute detection rate\n",
        "bin_stats = df_analysis.groupby(\"slant_bin\", observed=False)[\"detected\"].agg([\"mean\", \"count\"]).reset_index()\n",
        "bin_stats.rename(columns={\"mean\": \"detection_rate\", \"count\": \"num_images\"}, inplace=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=bin_stats, x=\"slant_bin\", y=\"detection_rate\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel(\"Slant Distance Bin (Km)\")\n",
        "plt.ylabel(\"Detection Rate\")\n",
        "\n",
        "title_name = DATASET_CONFIGS['synthetic']['name']\n",
        "plt.title(f\"Detection Rate vs Slant Distance ({title_name})\")\n",
        "\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OoaWlZOu6oLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "sns.lineplot(data=bin_stats, x=bin_stats.index, y=\"detection_rate\", marker='o')\n",
        "plt.xticks(ticks=bin_stats.index, labels=[str(b) for b in bin_stats[\"slant_bin\"]], rotation=45, ha='right')\n",
        "plt.title(\"Detection Rate Trend across Slant Distance\")\n",
        "plt.xlabel(\"Slant Distance Bin\")\n",
        "plt.ylabel(\"Detection Rate\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y6iKAuNn7qGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique bins we created in the previous step from the dataframe\n",
        "slant_bins = df_analysis[\"slant_bin\"].unique()\n",
        "\n",
        "# Sort the bins by their lower edge for a clean, ordered visualization\n",
        "slant_bins = sorted(slant_bins, key=lambda x: x.left)\n",
        "\n",
        "print(\"--- Visualizing examples from a few slant distance bins ---\")\n",
        "# Loop through a sample of the bins to visualize them (e.g., every 2nd bin)\n",
        "for bin_label in slant_bins[::2]:\n",
        "    # Create a temporary dataframe containing only the images from the current bin\n",
        "    bin_df = df_analysis[df_analysis[\"slant_bin\"] == bin_label]\n",
        "    if bin_df.empty:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüì∏ Visualizing Slant Distance Bin: {bin_label}\")\n",
        "\n",
        "    # Call our main collage function to display a sample of images from this bin\n",
        "    visualize_bin_collage(\n",
        "        bin_df,\n",
        "        bin_label=bin_label,\n",
        "        metric_col=\"slant_distance\",\n",
        "        metric_name=\"Slant Distance (km)\",\n",
        "        title_prefix=\"Slant Distance Bin\",\n",
        "        data=data,  # Pass the main data dictionary for metadata lookup\n",
        "        num_images=3,\n",
        "        cols=3\n",
        "    )"
      ],
      "metadata": {
        "id": "-MqBVQCI__Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection Rate vs Area of runway (in pixels)"
      ],
      "metadata": {
        "id": "I_AnpK2ejgsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing runway area for all datasets...\")\n",
        "for dataset_type in data:\n",
        "    # Get the dataframe from the dictionary\n",
        "    df = data[dataset_type]['df']\n",
        "    df['runway_area'] = df.apply(compute_quad_area, axis=1)\n",
        "\n",
        "# 2. MODIFIED CONCATENATION: Create the list of DataFrames from the 'data' dictionary.\n",
        "list_of_dfs = [data[dataset_type]['df'] for dataset_type in data]\n",
        "all_df = pd.concat(list_of_dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "7w9CVNNSJlJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Binning and plotting results...\")\n",
        "# Bin into quantile-based buckets across all datasets\n",
        "all_df['area_bin'] = pd.qcut(all_df['runway_area'], q=5, duplicates='drop')\n",
        "\n",
        "# Group by area bin and source\n",
        "area_stats = all_df.groupby(['area_bin', 'original_dataset'], observed=False)['detected'].mean().reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=area_stats, x='area_bin', y='detected', hue='original_dataset')\n",
        "plt.title('Detection Rate vs. Runway Area (Quantile Bins)')\n",
        "plt.xlabel('Runway Area (Binned)')\n",
        "plt.ylabel('Detection Rate')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ntbSiM1cKAJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "area_bins = all_df['area_bin'].cat.categories\n",
        "bins_to_visualize = [area_bins[0], area_bins[2], area_bins[4]]\n",
        "\n",
        "for bin_label in bins_to_visualize:\n",
        "    bin_df = all_df[all_df['area_bin'] == bin_label]\n",
        "    if bin_df.empty:\n",
        "        continue\n",
        "    visualize_bin_collage(\n",
        "        bin_df,\n",
        "        bin_label,\n",
        "        metric_col=\"runway_area\",\n",
        "        metric_name=\"Area\",\n",
        "        title_prefix=\"Runway Area Bin\",\n",
        "        data=data # Pass the data dictionary here\n",
        "    )"
      ],
      "metadata": {
        "id": "Dnw-20e37Iai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detection Rate vs Time to Landing (TTL)"
      ],
      "metadata": {
        "id": "imzlAhI2kVV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_real = pd.concat([data['nominal']['df'], data['edge']['df']], ignore_index=True)\n",
        "\n",
        "# Drop rows with missing TTL or detection\n",
        "df_real = df_real.dropna(subset=['time_to_landing', 'detected'])\n",
        "\n",
        "# Convert to numeric\n",
        "df_real['time_to_landing'] = pd.to_numeric(df_real['time_to_landing'])"
      ],
      "metadata": {
        "id": "ZU9u0sBBkmHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the width (in seconds) for each time-to-landing bin\n",
        "bin_width = 6\n",
        "\n",
        "# Find the maximum TTL value in the dataset to determine the upper range for our bins\n",
        "max_ttl = df_real['time_to_landing'].max()\n",
        "\n",
        "# Create a list of bin edges, starting from 0 up to the max TTL, in steps of bin_width\n",
        "# We add an extra bin at the end to ensure the max value is included.\n",
        "bins = list(range(0, int(max_ttl) + bin_width * 2, bin_width))\n",
        "\n",
        "# Create corresponding labels for each bin (e.g., \"0-6\", \"6-12\")\n",
        "labels = [f\"{b}-{b+bin_width}\" for b in bins[:-1]]\n",
        "\n",
        "# Use pandas.cut to categorize each row's TTL value into the appropriate bin\n",
        "df_real['ttl_bin'] = pd.cut(df_real['time_to_landing'], bins=bins, labels=labels, right=False)"
      ],
      "metadata": {
        "id": "UTlMWo83MpHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the TTL bin and the original dataset to calculate the mean detection rate for each subgroup\n",
        "ttl_stats = df_real.groupby(['ttl_bin', 'original_dataset'], observed=True)['detected'].mean().reset_index()\n",
        "\n",
        "# Group by the same categories to get the number of images in each subgroup\n",
        "ttl_counts = df_real.groupby(['ttl_bin', 'original_dataset'], observed=True).size().reset_index(name='count')"
      ],
      "metadata": {
        "id": "WjNdkVNyMrMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by TTL bin to get the image count for the Nominal dataset\n",
        "ttl_bin_counts_nominal = df_real[df_real['original_dataset'] == 'REAL_Nominal'].groupby(['ttl_bin'], observed=True).size().reset_index(name='count')\n",
        "# Do the same for the Edge Cases dataset\n",
        "ttl_bin_counts_edge = df_real[df_real['original_dataset'] == 'REAL_Edge_Cases'].groupby(['ttl_bin'], observed=True).size().reset_index(name='count')\n",
        "\n",
        "# Merge the two count dataframes into a single table for comparison\n",
        "combined_ttl_counts = pd.merge(\n",
        "    ttl_bin_counts_nominal,\n",
        "    ttl_bin_counts_edge,\n",
        "    on='ttl_bin',\n",
        "    suffixes=('_nominal', '_edge'),\n",
        "    how='outer' # Use an outer join to make sure no bins are dropped\n",
        ")\n",
        "\n",
        "# Clean up the merged data by filling any missing values (NaN) with 0\n",
        "combined_ttl_counts[['count_nominal', 'count_edge']] = combined_ttl_counts[['count_nominal', 'count_edge']].fillna(0)\n",
        "\n",
        "# Convert the count columns to integers for a cleaner look\n",
        "combined_ttl_counts['count_nominal'] = combined_ttl_counts['count_nominal'].astype(int)\n",
        "combined_ttl_counts['count_edge'] = combined_ttl_counts['count_edge'].astype(int)\n",
        "\n",
        "\n",
        "# Calculate the total number of images per bin\n",
        "combined_ttl_counts['total_count'] = combined_ttl_counts['count_nominal'] + combined_ttl_counts['count_edge']\n",
        "\n",
        "# Rename columns for better readability in the final table\n",
        "combined_ttl_counts.rename(columns={\n",
        "    'count_nominal': 'Real-Nominal Count',\n",
        "    'count_edge': 'Real-Edge Count'\n",
        "}, inplace=True)\n",
        "\n",
        "# Sort the table by the TTL bins to ensure it's in chronological order\n",
        "# This is important because 'pd.cut' creates a categorical type that has a defined order\n",
        "combined_ttl_counts['ttl_bin'] = pd.Categorical(\n",
        "    combined_ttl_counts['ttl_bin'],\n",
        "    categories=df_real['ttl_bin'].cat.categories,\n",
        "    ordered=True\n",
        ")\n",
        "combined_ttl_counts = combined_ttl_counts.sort_values('ttl_bin')\n",
        "\n",
        "\n",
        "# Display the final, formatted table\n",
        "print(combined_ttl_counts.to_string(index=False))"
      ],
      "metadata": {
        "id": "o6gh0iQmj-Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttl_stats_filtered = ttl_stats[ttl_stats['ttl_bin'].isin(combined_ttl_counts['ttl_bin'])]"
      ],
      "metadata": {
        "id": "_YylHjRd8bxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax_plot, ax_table) = plt.subplots(\n",
        "    1, 2,\n",
        "    figsize=(16, 8),\n",
        "    gridspec_kw={'width_ratios': [3, 1]}  # Wider plot, narrower table\n",
        ")\n",
        "\n",
        "# Bar Plot (left)\n",
        "default_palette = sns.color_palette()\n",
        "palette = {\n",
        "    'REAL_Nominal': default_palette[1],\n",
        "    'REAL_Edge_Cases': default_palette[0]\n",
        "}\n",
        "\n",
        "sns.barplot(\n",
        "    data=ttl_stats_filtered,\n",
        "    x='ttl_bin',\n",
        "    y='detected',\n",
        "    hue='original_dataset',\n",
        "    hue_order=['REAL_Nominal', 'REAL_Edge_Cases'],\n",
        "    palette=palette,\n",
        "    ax=ax_plot\n",
        ")\n",
        "\n",
        "ax_plot.set_xlabel('Time to Landing (seconds)')\n",
        "ax_plot.set_ylabel('Detection Rate')\n",
        "ax_plot.set_title('Detection Rate vs Time to Landing')\n",
        "ax_plot.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# --- Table (right) ---\n",
        "# Prepare table data\n",
        "table_data = [\n",
        "    [str(bin_label), nom, edge] for bin_label, nom, edge in zip(\n",
        "        combined_ttl_counts['ttl_bin'],\n",
        "        combined_ttl_counts['Real-Nominal Count'],\n",
        "        combined_ttl_counts['Real-Edge Count']\n",
        "    )\n",
        "]\n",
        "\n",
        "# Add total row\n",
        "total_nominal = combined_ttl_counts['Real-Nominal Count'].sum()\n",
        "total_edge = combined_ttl_counts['Real-Edge Count'].sum()\n",
        "table_data.append([\"Total\", total_nominal, total_edge])\n",
        "\n",
        "# Hide table axes\n",
        "ax_table.axis('off')\n",
        "\n",
        "# Create the table\n",
        "table = ax_table.table(\n",
        "    cellText=table_data,\n",
        "    colLabels=[\"TTL Bin\", \"Nominal\", \"Edge\"],\n",
        "    cellLoc='center',\n",
        "    loc='center'\n",
        ")\n",
        "\n",
        "# Style table\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlTyR8UORAJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Infer ttl for synthetic if possible"
      ],
      "metadata": {
        "id": "h7XSdGtvBe9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Failure Analysis"
      ],
      "metadata": {
        "id": "_95XJGR7B5vV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1d11843"
      },
      "source": [
        "print(\"--- Calculating and Visualizing False Negatives ---\")\n",
        "\n",
        "# Calculate and Print Stats\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    df = data[dataset_type]['df']\n",
        "    false_negatives_count = len(df[df['detected'] == False])\n",
        "    total_count = len(df)\n",
        "    dataset_name = config.get('name', dataset_type)\n",
        "    print(f\"({dataset_name}): {false_negatives_count} / {total_count} false negatives\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a subplot grid to display samples from each dataset\n",
        "num_datasets = len(DATASET_CONFIGS)\n",
        "num_samples = 3\n",
        "fig, axs = plt.subplots(num_datasets, num_samples, figsize=(num_samples * 5, num_datasets * 5))\n",
        "\n",
        "# Ensure axs is always a 2D array, even if there's only one dataset\n",
        "if num_datasets == 1:\n",
        "    axs = axs.reshape(1, -1)\n",
        "\n",
        "# Loop through each dataset to find and visualize its false negatives\n",
        "for row_idx, (dataset_type, config) in enumerate(DATASET_CONFIGS.items()):\n",
        "\n",
        "    # Get the necessary data for this dataset from our central dictionaries\n",
        "    df = data[dataset_type]['df']\n",
        "    gt_data = data[dataset_type]['gt']\n",
        "    image_folder = config['image_folder']\n",
        "    display_name = config['name']\n",
        "\n",
        "    # Filter the DataFrame to get only the rows corresponding to false negatives\n",
        "    df_fn = df[df['detected'] == False]\n",
        "\n",
        "    # If there are no false negatives for this dataset, skip to the next one\n",
        "    if df_fn.empty:\n",
        "        print(f\"\\nNo false negatives to visualize for {display_name}.\")\n",
        "        # Turn off the axes for this row of the plot\n",
        "        for col_idx in range(num_samples):\n",
        "            axs[row_idx, col_idx].axis('off')\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüì∏ Visualizing False Negatives for: {display_name}\")\n",
        "    # Take a random, sample of the false negative images to display\n",
        "    sample_rows = df_fn.sample(min(num_samples, len(df_fn)), random_state=42)\n",
        "\n",
        "    # Loop through the sampled images and draw them on the plot\n",
        "    for col_idx, (_, row) in enumerate(sample_rows.iterrows()):\n",
        "        img_name = row['image_name']\n",
        "        orig_w, orig_h = row['width'], row['height']\n",
        "\n",
        "        # Get the image path and ground truth boxes for this image\n",
        "        img_path = os.path.join(image_folder, img_name)\n",
        "        gt_boxes = gt_data.get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot for this image\n",
        "        ax = axs[row_idx, col_idx]\n",
        "\n",
        "        # Call the main drawing function to display the image and its ground truth\n",
        "        draw_predictions_with_gt(\n",
        "            img_path=img_path,\n",
        "            gt_boxes_orig=gt_boxes,\n",
        "            pred_boxes=[],  # Pass empty lists for predictions, as these are FNs\n",
        "            pred_scores=[],\n",
        "            orig_w=orig_w,\n",
        "            orig_h=orig_h,\n",
        "            ax=ax,\n",
        "            dataset_name=display_name\n",
        "        )\n",
        "\n",
        "# Clean up the plot by hiding any subplots that were not used\n",
        "for row in axs:\n",
        "    for ax in row:\n",
        "        if not ax.has_data():\n",
        "            ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1kSE7uUyiYDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images with multiple detections"
      ],
      "metadata": {
        "id": "Y_rNMrV3qn8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Calculating and Visualizing Multiple Detections ---\")\n",
        "\n",
        "all_multi_detections = []\n",
        "\n",
        "# Loop through datasets to get all multi-detection instances\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    outputs = data[dataset_type]['outputs']\n",
        "\n",
        "    multi_list, count = get_multiple_detections(outputs, score_thresh=0.90, detailed=True)\n",
        "\n",
        "    print(f\"multiple detections in ({config['name']}): {count}\")\n",
        "\n",
        "    # Tag each result with its dataset_type and add to our master list\n",
        "    for img_name, boxes, scores in multi_list:\n",
        "        all_multi_detections.append((img_name, boxes, scores, dataset_type))"
      ],
      "metadata": {
        "id": "HvLIV2bFt6xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not all_multi_detections:\n",
        "    print(\"\\nNo instances of multiple detections found to visualize.\")\n",
        "else:\n",
        "    num_images = min(len(all_multi_detections), 9)\n",
        "    cols = 3\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    # Sample from the combined list\n",
        "    sampled_detections = random.sample(all_multi_detections, num_images)\n",
        "\n",
        "    # Plotting loop using the master list and central data structure\n",
        "    for idx, (img_name, pred_boxes, pred_scores, dataset_tag) in enumerate(sampled_detections):\n",
        "\n",
        "        # Get all metadata directly\n",
        "        img_name_cleaned = os.path.basename(img_name)\n",
        "        config = DATASET_CONFIGS[dataset_tag]\n",
        "        df = data[dataset_tag]['df']\n",
        "        gt_data = data[dataset_tag]['gt']\n",
        "\n",
        "        row_match = df[df[\"image_name\"] == img_name_cleaned]\n",
        "\n",
        "        if row_match.empty:\n",
        "            print(f\"[!] Metadata not found for {img_name_cleaned} in {dataset_tag}\")\n",
        "            continue\n",
        "\n",
        "        row = row_match.iloc[0]\n",
        "        orig_w, orig_h = row[\"width\"], row[\"height\"]\n",
        "        img_path = os.path.join(config['image_folder'], img_name_cleaned)\n",
        "        gt_boxes = gt_data.get(img_name_cleaned, [])\n",
        "\n",
        "        draw_predictions_with_gt(img_path, gt_boxes, pred_boxes, pred_scores, orig_w, orig_h, axs[idx], dataset_name=config['name'])\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for j in range(idx + 1, len(axs)):\n",
        "        axs[j].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XYD3QC-qjPav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## False Negatives\n",
        "\n"
      ],
      "metadata": {
        "id": "0aZmFVCN_xKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Running Evaluation on All Datasets ---\")\n",
        "\n",
        "all_results = {}\n",
        "for dataset_type, config in DATASET_CONFIGS.items():\n",
        "    print(f\"\\nEvaluating {config['name']}...\")\n",
        "    result = evaluate_dataset(dataset_type, score_threshold=0.90, iou_threshold=0.3)\n",
        "    all_results[config['name']] = result\n",
        "\n",
        "print(\"\\n--- Evaluation Summary ---\")\n",
        "summary_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
        "# Optional: format columns for better display\n",
        "summary_df[['precision', 'recall', 'f1_score']] = summary_df[['precision', 'recall', 'f1_score']].map('{:.3f}'.format)\n",
        "print(summary_df)"
      ],
      "metadata": {
        "id": "1-GE4P8T_Y8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "278a4619"
      },
      "source": [
        "all_per_image_results = []\n",
        "for dataset_type in DATASET_CONFIGS:\n",
        "    results = evaluate_dataset(dataset_type, return_detailed=True)\n",
        "    all_per_image_results.extend(results)\n",
        "\n",
        "print(\"Per-image evaluation results obtained for all datasets.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify all images containing at least one False Negative\n",
        "# We use the 'all_per_image_results' list from our detailed evaluation.\n",
        "fn_images = set()\n",
        "for result in all_per_image_results:\n",
        "    if any(result['is_gt_box_fn']):\n",
        "        fn_images.add(result['image_name'])\n",
        "\n",
        "print(f\"Found {len(fn_images)} unique images containing at least one False Negative.\")\n",
        "\n",
        "\n",
        "# Prepare a unified DataFrame for analysis\n",
        "# Concatenate all individual dataframes into one\n",
        "all_dfs_list = [data[dtype]['df'] for dtype in DATASET_CONFIGS]\n",
        "all_df = pd.concat(all_dfs_list, ignore_index=True)\n",
        "\n",
        "# Add a boolean column to flag the images that have False Negatives\n",
        "all_df['has_false_negative'] = all_df['image_name'].isin(fn_images)\n",
        "\n",
        "\n",
        "# Create a plotting function to compare distributions\n",
        "\n",
        "def plot_error_rate_by_feature(df, feature_col, error_col, num_bins=10, log_bins=False):\n",
        "    \"\"\"\n",
        "    Calculates and plots the error rate as a function of a binned feature.\n",
        "    (This version has corrected label formatting for float values)\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    if log_bins:\n",
        "        min_val = df_copy[df_copy[feature_col] > 0][feature_col].min()\n",
        "        max_val = df_copy[feature_col].max()\n",
        "        bins = np.logspace(np.log10(min_val), np.log10(max_val), num_bins + 1)\n",
        "    else:\n",
        "        bins = num_bins\n",
        "\n",
        "    df_copy['feature_bin'] = pd.cut(df_copy[feature_col], bins=bins, include_lowest=True, duplicates='drop')\n",
        "\n",
        "    total_counts = df_copy.groupby('feature_bin', observed=False).size()\n",
        "    error_counts = df_copy[df_copy[error_col] == True].groupby('feature_bin', observed=False).size()\n",
        "\n",
        "    error_rate = (error_counts / total_counts).fillna(0) * 100\n",
        "    error_rate_df = error_rate.reset_index(name='error_rate')\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    ax = sns.barplot(data=error_rate_df, x='feature_bin', y='error_rate', color='#4c72b0')\n",
        "\n",
        "    # Format the labels as floats with 2 decimal places instead of integers\n",
        "    if log_bins:\n",
        "        new_labels = [f\"{int(b.left)}-{int(b.right)}\" for b in error_rate_df['feature_bin']]\n",
        "    else:\n",
        "        new_labels = [f\"{b.left:.2f}-{b.right:.2f}\" for b in error_rate_df['feature_bin']]\n",
        "\n",
        "    ax.set_xticks(ax.get_xticks())\n",
        "    ax.set_xticklabels(new_labels, rotation=45, ha='right')\n",
        "\n",
        "    plt.xlabel(f\"{feature_col.replace('_', ' ').title()} Bins\")\n",
        "    plt.ylabel(f\"{error_col.replace('has_', '').replace('_', ' ').title()} Rate (%)\")\n",
        "    plt.title(f\"Model Failure Rate vs. {feature_col.replace('_', ' ').title()}\", fontsize=16)\n",
        "\n",
        "    for i, p in enumerate(ax.patches):\n",
        "        total_in_bin = total_counts.iloc[i]\n",
        "        rate_text = f'{p.get_height():.1f}%'\n",
        "        count_text = f'n={total_in_bin}'\n",
        "\n",
        "        ax.text(p.get_x() + p.get_width() / 2., p.get_height(), rate_text,\n",
        "                fontsize=11, color='black', ha='center', va='bottom')\n",
        "        ax.text(p.get_x() + p.get_width() / 2., p.get_height()/2, count_text,\n",
        "                fontsize=9, color='white', ha='center', va='center', weight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oY0YoMwSdJQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze and Plot 'runway_area'\n",
        "print(\"\\n--- Analysis of Runway Area ---\")\n",
        "# Use .describe() to get a statistical summary for both groups\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None,'display.max_colwidth', None):\n",
        "    display(all_df.groupby('has_false_negative')['runway_area'].describe())\n",
        "\n",
        "plot_error_rate_by_feature(\n",
        "    df=all_df,\n",
        "    feature_col='runway_area',\n",
        "    error_col='has_false_negative',\n",
        "    num_bins=8, # You can adjust the number of bins\n",
        "    log_bins=True\n",
        ")"
      ],
      "metadata": {
        "id": "T2BbMH55fHEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze and Plot 'slant_distance' (for synthetic data)\n",
        "print(\"\\n--- Analysis of Slant Distance (Synthetic Dataset) ---\")\n",
        "# Slant distance only exists for the synthetic dataset\n",
        "synth_df = all_df[all_df['original_dataset'] == 'LARD_test_synth'].copy()\n",
        "synth_df['slant_distance'] = pd.to_numeric(synth_df['slant_distance'], errors='coerce')\n",
        "synth_df.dropna(subset=['slant_distance'], inplace=True)\n",
        "\n",
        "# Call the plotting function for the 'slant_distance' feature\n",
        "# We set log_bins=False because this feature is not skewed over many orders of magnitude\n",
        "plot_error_rate_by_feature(\n",
        "    df=synth_df,\n",
        "    feature_col='slant_distance',\n",
        "    error_col='has_false_negative',\n",
        "    num_bins=8,\n",
        "    log_bins=False\n",
        ")"
      ],
      "metadata": {
        "id": "oVyygKeheUIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for the specific outlier cases\n",
        "all_df['slant_distance'] = pd.to_numeric(all_df['slant_distance'], errors='coerce')\n",
        "\n",
        "fn_low_distance_df = all_df[\n",
        "    (all_df['original_dataset'] == 'LARD_test_synth') &\n",
        "    (all_df['has_false_negative'] == True) &\n",
        "    (all_df['slant_distance'] < 1.5)\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(f\"Found {len(fn_low_distance_df)} False Negative images with slant distance < 1.5 km.\")\n",
        "\n",
        "\n",
        "# Visualize a sample of these outlier images\n",
        "if not fn_low_distance_df.empty:\n",
        "    num_samples = min(len(fn_low_distance_df), 9)\n",
        "    samples_to_viz = fn_low_distance_df.sample(num_samples, random_state=42)\n",
        "\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(f\"\\n--- Visualizing {num_samples} sample outlier False Negatives ---\")\n",
        "\n",
        "    for idx, (_, row) in enumerate(samples_to_viz.iterrows()):\n",
        "        # Get all necessary info directly from the sampled row and our central data\n",
        "        img_name = row['image_name']\n",
        "        slant_dist = row['slant_distance']\n",
        "        orig_w, orig_h = row['width'], row['height']\n",
        "        dataset_type = 'synthetic' # We know this from our filter\n",
        "\n",
        "        # Retrieve data from our central dictionaries\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "        preds = data[dataset_type]['outputs'].get(img_name, {})\n",
        "        pred_boxes = preds.get('boxes', [])\n",
        "        pred_scores = preds.get('scores', [])\n",
        "\n",
        "        # Call drawing function\n",
        "        ax = axs[idx]\n",
        "        draw_predictions_with_gt(\n",
        "            img_path=img_path,\n",
        "            gt_boxes_orig=gt_boxes,\n",
        "            pred_boxes=pred_boxes,\n",
        "            pred_scores=pred_scores,\n",
        "            orig_w=orig_w,\n",
        "            orig_h=orig_h,\n",
        "            ax=ax,\n",
        "            dataset_name=config['name']\n",
        "        )\n",
        "        ax.set_title(f\"{img_name}\\nSlant Distance: {slant_dist:.2f} km\", fontsize=9)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(idx + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo False Negative images found matching the specified criteria.\")"
      ],
      "metadata": {
        "id": "9hY5tGf4nnrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## False Positives"
      ],
      "metadata": {
        "id": "VaYkt_xcmo8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Analyzing Images with at Least One False Positive ---\")\n",
        "\n",
        "# Initialize everything we need\n",
        "images_with_fp = [] # The list of detailed results for images with FPs\n",
        "fp_images_nominal = set()\n",
        "fp_images_edge = set()\n",
        "fp_images_synth = set()\n",
        "\n",
        "# Iterate through all the detailed results\n",
        "for result in all_per_image_results:\n",
        "    # Check for the False Positive condition\n",
        "    if False in result['is_predicted_box_tp']:\n",
        "\n",
        "        # Add the detailed result to our main list for later use\n",
        "        images_with_fp.append(result)\n",
        "\n",
        "        # Add the image name to the correct set for counting\n",
        "        image_name = result['image_name']\n",
        "        dataset_name = result['dataset_name']\n",
        "\n",
        "        if dataset_name == 'REAL_Nominal':\n",
        "            fp_images_nominal.add(image_name)\n",
        "        elif dataset_name == 'REAL_Edge_Cases':\n",
        "            fp_images_edge.add(image_name)\n",
        "        elif dataset_name == 'LARD_test_synth':\n",
        "            fp_images_synth.add(image_name)\n",
        "\n",
        "# Print the breakdown of counts\n",
        "print(f\"Real-Nominal Dataset: {len(fp_images_nominal)} images\")\n",
        "print(f\"Real-Edge Dataset:    {len(fp_images_edge)} images\")\n",
        "print(f\"Synthetic Dataset:    {len(fp_images_synth)} images\")\n",
        "print(f\"\\nTotal unique images with False Positives: {len(images_with_fp)}\")\n",
        "\n",
        "\n",
        "# Update the main DataFrame\n",
        "all_fp_images = fp_images_nominal | fp_images_edge | fp_images_synth\n",
        "all_df['has_false_positive'] = all_df['image_name'].isin(all_fp_images)\n",
        "print(\"DataFrame 'all_df' has been updated with the 'has_false_positive' column.\")"
      ],
      "metadata": {
        "id": "ENrxg9CbhOqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_detailed_predictions(ax, img_path, gt_boxes, result_dict, target_size=1024):\n",
        "    \"\"\"\n",
        "    Draws ground truth boxes and color-coded model predictions on an image.\n",
        "    - Ground Truth: Green\n",
        "    - True Positives (TP): Blue\n",
        "    - False Positives (FP): Red\n",
        "    \"\"\"\n",
        "    # Load and resize the image for display\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    orig_w, orig_h = img.size\n",
        "    img = img.resize((target_size, target_size))\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Scale and draw the ground truth boxes in green\n",
        "    scaled_gt_boxes = scale_boxes(gt_boxes, orig_w, orig_h, target_size, target_size)\n",
        "    for box in scaled_gt_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='lime', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    # Get the model's predictions and their corresponding TP/FP flags\n",
        "    pred_boxes = result_dict['predicted_boxes']\n",
        "    is_tp_flags = result_dict['is_predicted_box_tp']\n",
        "\n",
        "    # Loop through each prediction and draw it with the correct color\n",
        "    for box, is_tp in zip(pred_boxes, is_tp_flags):\n",
        "        # Set color to blue for True Positives, red for False Positives\n",
        "        color = 'blue' if is_tp else 'red'\n",
        "        x1, y1, x2, y2 = box\n",
        "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, linestyle='--', edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)"
      ],
      "metadata": {
        "id": "_Lokdl17m-ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the detailed results to find images that have both a correct detection (TP)\n",
        "# and an incorrect detection (FP).\n",
        "images_with_tp_and_fp = []\n",
        "for result in all_per_image_results:\n",
        "    has_tp = True in result['is_predicted_box_tp']\n",
        "    has_fp = False in result['is_predicted_box_tp']\n",
        "    if has_tp and has_fp:\n",
        "        images_with_tp_and_fp.append(result)\n",
        "\n",
        "print(f\"Found {len(images_with_tp_and_fp)} unique images containing both True Positives and False Positives.\")\n",
        "\n",
        "\n",
        "# Visualize a random sample of these images if any were found\n",
        "if images_with_tp_and_fp:\n",
        "    # Set up the visualization grid\n",
        "    num_samples = min(len(images_with_tp_and_fp), 9)\n",
        "    samples_to_viz = random.sample(images_with_tp_and_fp, num_samples)\n",
        "\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(f\"\\n--- Visualizing {num_samples} sample images with both TPs and FPs ---\")\n",
        "    print(\"Green = Ground Truth | Blue = True Positive | Red = False Positive\")\n",
        "\n",
        "    # Loop through the sampled images and plot them\n",
        "    for idx, result in enumerate(samples_to_viz):\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        # Find the short dataset key (e.g., 'nominal') from its full name\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Get the necessary image path and ground truth data\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot and draw the image with annotations\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name_from_result}\", fontsize=9)\n",
        "\n",
        "        # Manually create a legend for each subplot for clarity\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='blue', label='True Positive'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='small')\n",
        "\n",
        "\n",
        "    # Hide any unused subplots in the grid\n",
        "    for j in range(idx + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo images found containing both TPs and FPs.\")"
      ],
      "metadata": {
        "id": "8J84FEdmm_gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis of FP"
      ],
      "metadata": {
        "id": "Dpz0n-YRmwYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Analysis of Runway Area for False Positives ---\")\n",
        "\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None,'display.max_colwidth', None):\n",
        "    display(all_df.groupby('has_false_positive')['runway_area'].describe())\n",
        "\n",
        "# We use 'plot_error_rate_by_feature' to see how the FP rate changes with area.\n",
        "plot_error_rate_by_feature(\n",
        "    df=all_df,\n",
        "    feature_col='runway_area',\n",
        "    error_col='has_false_positive',\n",
        "    num_bins=8,\n",
        "    log_bins=True\n",
        ")"
      ],
      "metadata": {
        "id": "TMDWt23QhSZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze and Plot 'slant_distance' (for synthetic data) ---\n",
        "print(\"\\n--- Analysis of Slant Distance for False Positives (Synthetic Dataset) ---\")\n",
        "# Slant distance only exists for the synthetic dataset\n",
        "synth_df_fp = all_df[all_df['original_dataset'] == 'LARD_test_synth'].copy()\n",
        "if 'slant_distance' in synth_df_fp.columns:\n",
        "    # Add the FP flag specifically to this subset\n",
        "    synth_df_fp['has_false_positive'] = synth_df_fp['image_name'].isin(all_fp_images)\n",
        "    synth_df_fp['slant_distance'] = pd.to_numeric(synth_df_fp['slant_distance'], errors='coerce')\n",
        "    synth_df_fp.dropna(subset=['slant_distance'], inplace=True)\n",
        "\n",
        "    display(synth_df_fp.groupby('has_false_positive')['slant_distance'].describe())\n",
        "    plot_error_rate_by_feature(\n",
        "        df=synth_df_fp,\n",
        "        feature_col='slant_distance',\n",
        "        error_col='has_false_positive',\n",
        "        num_bins=8,\n",
        "        log_bins=False\n",
        "    )"
      ],
      "metadata": {
        "id": "e9_vpPXShR5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random sample of images to visualize\n",
        "num_samples = min(len(images_with_fp), 21)\n",
        "samples_for_viz = random.sample(images_with_fp, num_samples)\n",
        "\n",
        "# Create figure for plotting\n",
        "cols = 3\n",
        "rows = (num_samples + cols - 1) // cols\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
        "axs = axs.flatten()\n",
        "\n",
        "print(f\"\\n--- Visualizing {num_samples} sample images with False Positives ---\")\n",
        "\n",
        "# Iterate through the samples and plot\n",
        "for idx, result in enumerate(samples_for_viz):\n",
        "    # Get image name and dataset name directly from our rich result object\n",
        "    img_name = result['image_name']\n",
        "    dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "    # Find the corresponding 'dataset_type' key (e.g., 'nominal')\n",
        "    dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "    if not dataset_type:\n",
        "        axs[idx].axis('off')\n",
        "        continue\n",
        "\n",
        "    # Get all other data directly from our central data structures\n",
        "    config = DATASET_CONFIGS[dataset_type]\n",
        "    df = data[dataset_type]['df']\n",
        "    row_match = df[df['image_name'] == img_name]\n",
        "\n",
        "    if row_match.empty:\n",
        "        axs[idx].axis('off')\n",
        "        continue\n",
        "\n",
        "    row = row_match.iloc[0]\n",
        "    orig_w, orig_h = row['width'], row['height']\n",
        "    img_path = os.path.join(config['image_folder'], img_name)\n",
        "    gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "    # Get the original predictions for this image to draw them all\n",
        "    preds = data[dataset_type]['outputs'].get(img_name, {})\n",
        "    pred_boxes = preds.get('boxes', [])\n",
        "    pred_scores = preds.get('scores', [])\n",
        "\n",
        "    # Call drawing function\n",
        "    ax = axs[idx]\n",
        "    draw_predictions_with_gt(\n",
        "        img_path=img_path,\n",
        "        gt_boxes_orig=gt_boxes,\n",
        "        pred_boxes=pred_boxes,\n",
        "        pred_scores=pred_scores,\n",
        "        orig_w=orig_w,\n",
        "        orig_h=orig_h,\n",
        "        ax=ax,\n",
        "        dataset_name=config['name']\n",
        "    )\n",
        "    ax.set_title(f\"{img_name}\\nDataset: {config['name']}\", fontsize=9)\n",
        "\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(idx + 1, len(axs)):\n",
        "    axs[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-a8Ufdt5FaoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge case: Image with no TP but FP (also FN)"
      ],
      "metadata": {
        "id": "W2SYiGx1FvEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_images_preds_no_tp_fn(per_image_results):\n",
        "    \"\"\"\n",
        "    Filters for a specific failure case: images where the model made predictions (FPs),\n",
        "    missed the ground truth (FNs), but got zero correct detections (no TPs).\n",
        "    \"\"\"\n",
        "    filtered_list = []\n",
        "    # Go through each image's detailed results\n",
        "    for img_result in per_image_results:\n",
        "        # Check for at least one prediction\n",
        "        has_predictions = len(img_result['predicted_boxes']) > 0\n",
        "        # Check that none of the predictions were True Positives\n",
        "        has_no_tp = not any(img_result['is_predicted_box_tp'])\n",
        "        # Check that there was at least one ground truth that was missed\n",
        "        has_fn_gt = any(img_result['is_gt_box_fn'])\n",
        "\n",
        "        # If all conditions for this specific failure case are met, add it to our list\n",
        "        if has_predictions and has_no_tp and has_fn_gt:\n",
        "            filtered_list.append(img_result)\n",
        "\n",
        "    return filtered_list\n",
        "\n",
        "\n",
        "# Apply the filter function once to the combined list of all image results\n",
        "filtered_results = filter_images_preds_no_tp_fn(all_per_image_results)\n",
        "\n",
        "\n",
        "# Summarize the results dynamically using a Counter to count failures per dataset\n",
        "dataset_counts = Counter(result['dataset_name'] for result in filtered_results)\n",
        "\n",
        "# Print the breakdown of counts for each dataset\n",
        "print(f\"\\nNumber of images with predictions, no TP, and FN (Nominal): {dataset_counts.get('REAL_Nominal', 0)}\")\n",
        "print(f\"Number of images with predictions, no TP, and FN (Edge): {dataset_counts.get('REAL_Edge_Cases', 0)}\")\n",
        "print(f\"Number of images with predictions, no TP, and FN (Synthetic): {dataset_counts.get('LARD_test_synth', 0)}\")\n",
        "print(f\"Total number of unique images with this failure case: {len(filtered_results)}\")\n",
        "\n",
        "\n",
        "# Display a few sample image names from the filtered list\n",
        "if filtered_results:\n",
        "    print(\"\\nSample images with this failure case:\")\n",
        "    for result in filtered_results[:5]:\n",
        "        print(f\"- {result['image_name']} (from {result['dataset_name']})\")\n",
        "else:\n",
        "    print(\"\\nNo images found with predictions, no TP, and FN.\")"
      ],
      "metadata": {
        "id": "gd7Td4D7Eu-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727c0469"
      },
      "source": [
        "num_samples = min(len(filtered_results), 9)\n",
        "\n",
        "samples_for_viz = random.sample(filtered_results, num_samples)\n",
        "\n",
        "# 2. Create figure for plotting\n",
        "cols = 3\n",
        "rows = (num_samples + cols - 1) // cols\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "axs = axs.flatten()\n",
        "\n",
        "print(f\"--- Visualizing {num_samples} sample images with FP and FN ---\")\n",
        "\n",
        "# 3. Iterate through the samples and plot\n",
        "for idx, result in enumerate(samples_for_viz):\n",
        "    # Get image name and dataset tag\n",
        "    img_name = result['image_name']\n",
        "    dataset_name_from_result = result['dataset_name'] # e.g., 'REAL_Nominal'\n",
        "\n",
        "    # Find the corresponding 'dataset_type' key (e.g., 'nominal')\n",
        "    dataset_type = None\n",
        "    for key, conf in DATASET_CONFIGS.items():\n",
        "        if conf['name'] == dataset_name_from_result:\n",
        "            dataset_type = key\n",
        "            break\n",
        "\n",
        "    if not dataset_type:\n",
        "        axs[idx].axis('off')\n",
        "        continue\n",
        "\n",
        "    # Get all other data directly from our central data structures\n",
        "    config = DATASET_CONFIGS[dataset_type]\n",
        "    df = data[dataset_type]['df']\n",
        "    row_match = df[df['image_name'] == img_name]\n",
        "\n",
        "    if row_match.empty:\n",
        "        axs[idx].axis('off')\n",
        "        continue\n",
        "\n",
        "    row = row_match.iloc[0]\n",
        "    orig_w, orig_h = row['width'], row['height']\n",
        "    img_path = os.path.join(config['image_folder'], img_name)\n",
        "    gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "    # Get the original predictions for this image\n",
        "    preds = data[dataset_type]['outputs'].get(img_name, {})\n",
        "    pred_boxes = preds.get('boxes', [])\n",
        "    pred_scores = preds.get('scores', [])\n",
        "\n",
        "    # Call drawing function\n",
        "    ax = axs[idx]\n",
        "    draw_predictions_with_gt(\n",
        "        img_path=img_path,\n",
        "        gt_boxes_orig=gt_boxes,\n",
        "        pred_boxes=pred_boxes,\n",
        "        pred_scores=pred_scores,\n",
        "        orig_w=orig_w,\n",
        "        orig_h=orig_h,\n",
        "        ax=ax,\n",
        "        dataset_name=config['name']\n",
        "    )\n",
        "    ax.set_title(f\"{img_name}\\nDataset: {config['name']}\", fontsize=8)\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(idx + 1, len(axs)):\n",
        "    axs[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_duplicate_detections(detailed_results, iou_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Precisely identifies images where multiple predictions match the same\n",
        "    ground truth box.\n",
        "    \"\"\"\n",
        "    duplicate_cases = []\n",
        "\n",
        "    for result in detailed_results:\n",
        "        gt_boxes_orig = result.get('ground_truth_boxes', [])\n",
        "        pred_boxes = result.get('predicted_boxes', [])\n",
        "\n",
        "        if not gt_boxes_orig:\n",
        "            continue\n",
        "\n",
        "        img_name = result['image_name']\n",
        "        row_match = all_df[all_df['image_name'] == img_name]\n",
        "        if row_match.empty:\n",
        "            continue\n",
        "        orig_w, orig_h = row_match.iloc[0]['width'], row_match.iloc[0]['height']\n",
        "\n",
        "        # Call scale_boxes\n",
        "        scaled_gt_boxes = scale_boxes(gt_boxes_orig, orig_w, orig_h, 1024, 1024)\n",
        "\n",
        "        is_duplicate = False\n",
        "        for gt_box in scaled_gt_boxes:\n",
        "            matches = 0\n",
        "            for pred_box in pred_boxes:\n",
        "                if compute_iou(pred_box, gt_box) >= iou_threshold:\n",
        "                    matches += 1\n",
        "\n",
        "            if matches > 1:\n",
        "                is_duplicate = True\n",
        "                break\n",
        "\n",
        "        if is_duplicate:\n",
        "            duplicate_cases.append(result)\n",
        "\n",
        "    return duplicate_cases\n",
        "\n",
        "duplicate_detection_cases = find_duplicate_detections(all_per_image_results, iou_threshold=0.3)\n",
        "\n",
        "print(f\"Found {len(duplicate_detection_cases)} images with precise duplicate detections.\")"
      ],
      "metadata": {
        "id": "2Xth3R5YwDsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if duplicate_detection_cases:\n",
        "    num_samples = min(len(duplicate_detection_cases), 9)\n",
        "    samples_to_viz = random.sample(duplicate_detection_cases, num_samples)\n",
        "\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(f\"\\n--- Visualizing {num_samples} sample images with duplicate detections ---\")\n",
        "    print(\"Green = Ground Truth | Blue = Best Prediction (TP) | Red = Duplicate Prediction (FP)\")\n",
        "\n",
        "    for idx, result in enumerate(samples_to_viz):\n",
        "        # (The rest of your existing visualization code goes here and will work perfectly)\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name_from_result}\", fontsize=9)\n",
        "\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='blue', label='True Positive'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='small')\n",
        "\n",
        "    for j in range(idx + 1, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo images found matching this specific failure case.\")"
      ],
      "metadata": {
        "id": "xq6pOc6YzEH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_image_with_boxes(result, save_dir):\n",
        "    \"\"\"\n",
        "    Creates and saves an annotated image file for a given detection result.\n",
        "    It draws GT, TP, and FP boxes with dynamic line widths for clarity.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Get all the necessary metadata for the image from our central data stores\n",
        "        img_name = result['image_name']\n",
        "        dataset_name = result['dataset_name']\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name), None)\n",
        "\n",
        "        # If we can't find the dataset, skip this image\n",
        "        if not dataset_type: return\n",
        "\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes_orig = result.get('ground_truth_boxes', [])\n",
        "\n",
        "        # Set up a new matplotlib figure for this image\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        orig_w, orig_h = img.size\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Draw the ground truth boxes in green on the original image\n",
        "        for box in gt_boxes_orig:\n",
        "            x1, y1, x2, y2 = box\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            # Calculate a dynamic line width based on the box size\n",
        "            linewidth = max(1, min(5, (w**2 + h**2)**0.5 / 150))\n",
        "            rect = patches.Rectangle((x1, y1), w, h, linewidth=linewidth, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        # Draw the model's prediction boxes (Blue for TP, Red for FP)\n",
        "        pred_boxes_model_space = result.get('predicted_boxes', [])\n",
        "        # Scale the predictions from model space (1024x1024) back to original image coordinates\n",
        "        pred_boxes_orig_space = scale_boxes(pred_boxes_model_space, orig_w=1024, orig_h=1024, target_w=orig_w, target_h=orig_h)\n",
        "        is_tp_flags = result.get('is_predicted_box_tp', [])\n",
        "\n",
        "        for box, is_tp in zip(pred_boxes_orig_space, is_tp_flags):\n",
        "            color = 'blue' if is_tp else 'red'\n",
        "            x1, y1, x2, y2 = box\n",
        "            w, h = x2 - x1, y2 - y1\n",
        "            # Apply the same dynamic line width logic to the prediction boxes\n",
        "            linewidth = max(1, min(4, (w**2 + h**2)**0.5 / 150))\n",
        "            rect = patches.Rectangle((x1, y1), w, h, linewidth=linewidth, linestyle='--', edgecolor=color, facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        # Add title, legend, and save the final annotated image\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name}\", fontsize=12)\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='blue', label='True Positive'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='medium')\n",
        "\n",
        "        save_path = os.path.join(save_dir, img_name)\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n",
        "        # Close the figure to free up memory, which is important when processing many images\n",
        "        plt.close(fig)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during processing to prevent the whole script from crashing\n",
        "        print(f\"Failed to process image {img_name}. Error: {e}\")\n",
        "        plt.close('all') # Make sure to close any lingering plots on error"
      ],
      "metadata": {
        "id": "h2AFeZXy1_x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overlapping BBoxes"
      ],
      "metadata": {
        "id": "n1CygY0i6cIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific list of image filenames we want to visualize as examples\n",
        "image_filenames_to_visualize = [\n",
        "    '4s4zQFW4g7o_070.png',\n",
        "    'i8RhaLAUZ5g_074.png',\n",
        "    'LSZH_34_35_25.jpeg',\n",
        "    'LWSK_16_35_08.jpeg',\n",
        "    'rjWG3WRM5yk_067.png'\n",
        "]\n",
        "\n",
        "# Find the detailed result objects for these specific images from our master results list\n",
        "results_to_visualize = []\n",
        "for result in all_per_image_results:\n",
        "    if result['image_name'] in image_filenames_to_visualize:\n",
        "        results_to_visualize.append(result)\n",
        "\n",
        "print(f\"Found {len(results_to_visualize)} of the specified images for visualization.\")\n",
        "\n",
        "\n",
        "# Create the plot if we found any of the requested images\n",
        "if results_to_visualize:\n",
        "    # Set up the plot grid to display the images\n",
        "    num_samples = len(results_to_visualize)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(\"\\n--- Visualizing Specific Overlapping Detection Cases ---\")\n",
        "    print(\"Green = Ground Truth | Blue = True Positive | Red = False Positive (Overlap)\")\n",
        "\n",
        "    # Loop through our selected images and plot each one on a subplot\n",
        "    for idx, result in enumerate(results_to_visualize):\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        # Find the short dataset key (e.g., 'nominal') from its full name\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Get the image path and ground truth boxes\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot and call our drawing function\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name_from_result}\", fontsize=9)\n",
        "\n",
        "        # Add a legend to each subplot for clarity\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='blue', label='True Positive'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='small')\n",
        "\n",
        "    # Hide any empty subplots for a cleaner final image\n",
        "    for j in range(num_samples, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not find the specified images in the results list.\")"
      ],
      "metadata": {
        "id": "5QnIDoCl6feI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logos"
      ],
      "metadata": {
        "id": "Vj9_YSp-7GAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific list of image filenames to visualize as examples\n",
        "image_filenames_to_visualize = [\n",
        "    'yL_zy-9SDsg_093.png',\n",
        "    'yL_zy-9SDsg_089.png',\n",
        "    'yL_zy-9SDsg_087.png',\n",
        "    'LEMD_14L_35_21.jpeg',\n",
        "    '86ghu0Dcaog_109.png'\n",
        "]\n",
        "\n",
        "# Find the detailed result objects for these specific images from our master results list\n",
        "results_to_visualize = []\n",
        "for result in all_per_image_results:\n",
        "    if result['image_name'] in image_filenames_to_visualize:\n",
        "        results_to_visualize.append(result)\n",
        "\n",
        "print(f\"Found {len(results_to_visualize)} of the specified images for visualization.\")\n",
        "\n",
        "# Create the plot if we found any of the requested images\n",
        "if results_to_visualize:\n",
        "    # Set up the plot grid to display the images\n",
        "    num_samples = len(results_to_visualize)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(\"\\n--- Visualizing Specific 'Hallucination' Cases (Logos) ---\")\n",
        "    print(\"Green = Ground Truth | Red = False Positive\")\n",
        "\n",
        "    # Loop through our selected images and plot each one on a subplot\n",
        "    for idx, result in enumerate(results_to_visualize):\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        # Find the short dataset key (e.g., 'nominal') from its full name\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Get the image path and ground truth boxes\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot and call our drawing function\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name_from_result}\", fontsize=9)\n",
        "\n",
        "        # Add a legend to each subplot for clarity\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='small')\n",
        "\n",
        "    # Hide any empty subplots for a cleaner final image\n",
        "    for j in range(num_samples, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not find the specified images in the results list.\")"
      ],
      "metadata": {
        "id": "nZv8tz-I7GFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extended"
      ],
      "metadata": {
        "id": "Ol-8QxYz7emo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific list of image filenames to visualize as examples\n",
        "image_filenames_to_visualize = [\n",
        "    'CC-lSLLiueE_036.png',\n",
        "    'fB33KKArerU_059.png',\n",
        "    'JgKWRjH2O10_069.png',\n",
        "    'KMIA_9_35_26.jpeg',\n",
        "    'LEMD_32R_35_18.jpeg',\n",
        "    'LWSK_16_35_08.jpeg'\n",
        "]\n",
        "\n",
        "# Find the detailed result objects for these specific images from our master results list\n",
        "results_to_visualize = []\n",
        "for result in all_per_image_results:\n",
        "    if result['image_name'] in image_filenames_to_visualize:\n",
        "        results_to_visualize.append(result)\n",
        "\n",
        "print(f\"Found {len(results_to_visualize)} of the specified images for visualization.\")\n",
        "\n",
        "# Create the plot if we found any of the requested images\n",
        "if results_to_visualize:\n",
        "    # Set up the plot grid to display the images\n",
        "    num_samples = len(results_to_visualize)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(\"\\n--- Visualizing Incomplete Ground Truth Cases ---\")\n",
        "    print(\"Green = Ground Truth (Incomplete) | Blue/Red = Model Prediction (More Complete)\")\n",
        "\n",
        "    # Loop through our selected images and plot each one on a subplot\n",
        "    for idx, result in enumerate(results_to_visualize):\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        # Find the short dataset key (e.g., 'nominal') from its full name\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Get the image path and ground truth boxes\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot and call our drawing function\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_name_from_result}\", fontsize=9)\n",
        "\n",
        "        # Add a legend to each subplot for clarity\n",
        "        handles = [patches.Patch(color='lime', label='Ground Truth'),\n",
        "                   patches.Patch(color='blue', label='True Positive'),\n",
        "                   patches.Patch(color='red', label='False Positive')]\n",
        "        ax.legend(handles=handles, fontsize='small')\n",
        "\n",
        "    # Hide any empty subplots for a cleaner final image\n",
        "    for j in range(num_samples, len(axs)):\n",
        "        axs[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nCould not find the specified images in the results list.\")"
      ],
      "metadata": {
        "id": "7bPKyl7f7hJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# control panel"
      ],
      "metadata": {
        "id": "RoH7ODq589QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the specific list of image filenames to visualize as examples of this failure case\n",
        "image_filenames_to_visualize = [\n",
        "    'sxzWZX_I9hE_120.png', 'sxzWZX_I9hE_121.png',\n",
        "    'sxzWZX_I9hE_122.png', 'sqD0iHHDHqo_074.png',\n",
        "    'pA2yimUN7VI_083.png', 'pA2yimUN7VI_084.png',\n",
        "    'JMomkYzMYpA_047.png', 'JMomkYzMYpA_049.png'\n",
        "]\n",
        "\n",
        "# Find the detailed result objects for these specific images using a list comprehension\n",
        "results_to_visualize = [res for res in all_per_image_results if res['image_name'] in image_filenames_to_visualize]\n",
        "\n",
        "print(f\"Found {len(results_to_visualize)} of the specified images for visualization.\")\n",
        "\n",
        "# Create the plot if we found any of the requested images\n",
        "if results_to_visualize:\n",
        "    # Set up the plot grid to display the images\n",
        "    num_samples = len(results_to_visualize)\n",
        "    cols = 4\n",
        "    rows = (num_samples + cols - 1) // cols\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 6, rows * 5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    print(\"\\n--- Visualizing 'Hallucination' Cases (Control Panels) ---\")\n",
        "    print(\"Red = False Positive\")\n",
        "\n",
        "    # Loop through our selected images and plot each one on a subplot\n",
        "    for idx, result in enumerate(results_to_visualize):\n",
        "        img_name = result['image_name']\n",
        "        dataset_name_from_result = result['dataset_name']\n",
        "\n",
        "        # Find the short dataset key (e.g., 'nominal') from its full name\n",
        "        dataset_type = next((key for key, conf in DATASET_CONFIGS.items() if conf['name'] == dataset_name_from_result), None)\n",
        "\n",
        "        if not dataset_type:\n",
        "            axs[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Get the image path and ground truth boxes (which will be empty for these cases)\n",
        "        config = DATASET_CONFIGS[dataset_type]\n",
        "        img_path = os.path.join(config['image_folder'], img_name)\n",
        "        gt_boxes = data[dataset_type]['gt'].get(img_name, [])\n",
        "\n",
        "        # Select the correct subplot and call our drawing function\n",
        "        ax = axs[idx]\n",
        "        draw_detailed_predictions(ax, img_path, gt_boxes, result)\n",
        "        ax.set_title(f\"{img_name}\\n{dataset_"
      ],
      "metadata": {
        "id": "kJEq0_gi9Avy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}